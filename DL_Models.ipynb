{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/ayenyeinsan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/ayenyeinsan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/ayenyeinsan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import preprocessing\n",
    "\n",
    "\n",
    "cleaned_data = preprocessing.data\n",
    "\n",
    "cleaned_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_data['type'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
    "    cleaned_data['posts'], cleaned_data['type'], test_size=0.2, random_state=42, stratify=cleaned_data['type']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Check class distribution in the full dataset\n",
    "print(cleaned_data['type'].value_counts(normalize=True) * 100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert train and test labels to Pandas Series for value counts\n",
    "train_labels = pd.Series(train_labels)\n",
    "test_labels = pd.Series(test_labels)\n",
    "\n",
    "# Check class balance in train and test sets\n",
    "print(\"Train set class distribution:\\n\", train_labels.value_counts(normalize=True) * 100)\n",
    "print(\"\\nTest set class distribution:\\n\", test_labels.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#beacuse of the class imbalance more \"I\" in the train set and test set, we need to compute the class weights\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import numpy as np\n",
    "\n",
    "# Convert labels to numeric values (0 = I, 1 = E)\n",
    "cleaned_data['type'] = cleaned_data['type'].map({'I': 0, 'E': 1})\n",
    "\n",
    "# Compute class weights correctly\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=cleaned_data['type'])\n",
    "\n",
    "# Convert to dictionary for model training\n",
    "class_weights_dict = {0: class_weights[0], 1: class_weights[1]}\n",
    "\n",
    "print(\"Class Weights:\", class_weights_dict)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading Bert model and tokenizer\n",
    "# Choose model (BERT)\n",
    "MODEL_NAME = \"bert-base-uncased\"\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "# Load BERT model with 2 output labels (Introvert & Extrovert)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "def tokenize_function(texts):\n",
    "    return tokenizer(list(texts), truncation=True, padding=True, max_length=512)\n",
    "\n",
    "train_encodings = tokenize_function(train_texts)\n",
    "test_encodings = tokenize_function(test_texts)\n",
    "\n",
    "# Convert to Hugging Face Dataset\n",
    "train_dataset = Dataset.from_dict({\n",
    "    'input_ids': train_encodings['input_ids'],\n",
    "    'attention_mask': train_encodings['attention_mask'],\n",
    "    'labels': list(train_labels)\n",
    "})\n",
    "\n",
    "test_dataset = Dataset.from_dict({\n",
    "    'input_ids': test_encodings['input_ids'],\n",
    "    'attention_mask': test_encodings['attention_mask'],\n",
    "    'labels': list(test_labels)\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=200,\n",
    "    load_best_model_at_end=True\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=None,  # You can define custom metrics if needed\n",
    "    compute_loss=class_weights  # Apply class-weighted loss\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preducting the personality type\n",
    "def predict_mbti(text):\n",
    "    inputs = tokenizer(text, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    prediction = torch.argmax(outputs.logits).item()\n",
    "    return \"Extrovert (E)\" if prediction == 1 else \"Introvert (I)\"\n",
    "\n",
    "# Example\n",
    "new_post = \"I love spending time alone reading books.\"\n",
    "print(predict_mbti(new_post))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
